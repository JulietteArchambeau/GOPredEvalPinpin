---
title: "Baypass analysis"
author: "Juliette Archambeau"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format: 
  html:
    toc: true
    toc-depth: 4
    code-fold: true
    page-layout: full
embed-resources: true
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

<style type="text/css">
body {
   font-size: 15px;
}
code.r{
  font-size: 11px;
}
pre {
  font-size: 11px
}

table {
  font-size: 10px
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 5,fig.height = 4,cache=F)
options(width = 300)
library(knitr)      # CRAN v1.26
library(tidyverse)  # CRAN v1.3.0
library(readxl)     # CRAN v1.3.1
library(xtable)
library(reshape2)
library(kableExtra)
library(here)
library(parallel)
library(magrittr)
library(janitor)
library(corrplot)

# my own function for building tables in reports
source(here("scripts/functions/kable_mydf.R"))
source(here("scripts/functions/extract_climatedt_metadata.R")) # extracting meta data of the climatic variables in ClimateDT

# Functions from the BayPass software
source(here("scripts/functions/baypass_utils.R"))
```


In this document, we identify candidate SNPs using the **gene-environment association analysis** provided by the **<span style="font-variant:small-caps;">BayPass</span> software**. 

In previous analyses based on a subset of the genomic data (i.e. only SNPs genotyped in the Infinium assay, that is 5,165 SNPs), we found that the results of the **standard covariate model** were more robust using the **Important Sampling (IS) approximation** (default) than with the MCMC algorithm. So we only use the IS approximation in this document. 



# Load and format data
 
## Genomic data

We use the imputed genomic datasets, in which the imputation of missing data wad done using the most common allele at each SNP within the main gene pool of the clone (see report `1_FormattingGenomicData.Rmd`).

Importantly, we **estimate the population covariance matrix** with the genomic dataset **with minor allele frequencies** (MAF) because MAF can be important to estimate the population genetic structure.

Then we **identify the candidate SNPs** with a genomic dataset **without MAF**. 

The genomic data for <span style="font-variant:small-caps;">BayPass</span> needs to be allele counts for each SNP in each of the population and has to be stored in a `.txt` file.

```{r LoadGenomicData, eval=F}
# Function to count the second allele
count_2nd_allele <- function(x) {2*length(x)-sum(x)}

# We generate the genomic data in BayPass format
lapply(c("withoutmaf","withmaf"), function(x){
  
geno <- read.csv(here(paste0("data/DryadRepo/ImputedGenomicData_AlleleCounts_",x,".csv")),
                          row.names = 1) %>% 
  t() %>% 
  as.data.frame() %>% 
  mutate(prov=str_sub(row.names(.),1,3)) %>% 
  dplyr::select(prov,everything()) %>% 
  group_by(prov) %>% 
  dplyr::summarize_all(.funs=c('sum', 'count_2nd_allele')) # calculate allele counts per population


# Counts of the minor allele
dfsum1  <- geno %>% 
  column_to_rownames(var="prov") %>% 
  dplyr::select(contains("sum")) %>%  
  t() 

# counts of the major allele
dfsum2  <- geno %>% 
  column_to_rownames(var="prov") %>% 
  dplyr::select(contains("count")) %>%  
  t() 

colnames(dfsum1) <- paste0(geno$prov,"1")
colnames(dfsum2) <- paste0(geno$prov,"2")

rownames(dfsum1) <- str_sub(rownames(dfsum1),1,-5)
rownames(dfsum2) <- str_sub(rownames(dfsum2),1,-18)

# identical(rownames(dfsum1), rownames(dfsum2)) # to check that the rownames (SNP names) are the same and in the same order

cbind(dfsum1,dfsum2) %>% 
  as_tibble %>% 
  dplyr::select(sort(tidyselect::peek_vars())) %>% 

# We save the files in the format required by BayPass
write.table(file = here(paste0("data/BayPassAnalysis/GenomicInputBayPass_",x,".txt")), 
            sep = " ",
            row.names = FALSE, 
            col.names = FALSE) # works either with .txt or no extension
  
})
```



## Climatic data


We load the population-specific climatic information for the climatic variables of interest.

```{r LoadClimaticData}
# Which set of climatic variables are we going to use?
clim_var <- readRDS(here("data/ClimaticData/NamesSelectedVariables.rds"))

# we load the population-specific climatic data of the climatic variables of interest
source(here("scripts/functions/generate_scaled_clim_datasets.R"))
clim_ref <- generate_scaled_clim_datasets(clim_var)[["clim_ref"]]

extract_climatedt_metadata(var_clim = clim_var) %>% 
  dplyr::select(label,description,unit) %>% 
  set_colnames(str_to_title(colnames(.))) %>% 
  kable_mydf()
```

```{r LoadEnvPopStructureData, eval=F}
# We save one table per covariate
for(i in c(clim_var)){
  clim_ref %>% 
  dplyr::select(all_of(i)) %>% 
  pull() %>% 
  t() %>% 
  write.table(file = here(paste0("data/BayPassAnalysis/EnvironmentalVariables/",i,".txt")), 
              sep = " ",
              row.names = FALSE, 
              col.names = FALSE)
}
```


# Population covariance matrix

## The core model

We estimate the (scaled) covariance matrix of population allele frequencies $\Omega$ resulting from their (possibly unknown and complex) shared history. For that, we use the core model mode of <span style="font-variant:small-caps;">BayPass</span>. According to <span style="font-variant:small-caps;">BayPass</span> manual, 'The core model depicted in Figure 1A might be viewed as a generalization of the model proposed by Nicholson *et al*. (2002) and was first proposed by Coop et al. (2010).' Here is the **Figure 1A from the <span style="font-variant:small-caps;">BayPass</span> manual:**

![](CoreModelBayPass.png)


## Estimation & visualization

We estimate the population covariance matrix using the genomic dataset not filtered for MAF.


```{bash EstimateOmegaRun1, eval=F}
# ** Bash script **
# =================

# Working directory: data/BayPassAnalysis/outputs
# First row: directory where Baypass software is

~/Bureau/baypass_2.2/sources/g_baypass \ 
    -gfile ../GenomicInputBayPass_withmaf.txt \
    -outprefix anacore \
    -seed 44
```


We visualize the matrix.

```{r VisualizeOmega, fig.height=7,fig.width=7, eval=T, results="hide"}
# Upload the estimated Omega matrix
omega <- as.matrix(read.table(here("data/BayPassAnalysis/outputs/anacore_mat_omega.out")))
pop_names <- clim_ref$pop
dimnames(omega)=list(pop_names,pop_names)

# Visualization of the matrix
# Using SVD decomposition
plot.omega(omega=omega,pop.names=pop_names)

# as a correlation plot
cor_mat=cov2cor(omega)
# corrplot(cor_mat,method="color",mar=c(2,1,2,2)+0.1,
# main=expression("Correlation map based on"~hat(Omega)))

# as a heatmap and hierarchical clustering tree (using the average agglomeration method)
hclust_ave <- function(x) hclust(x, method="average")
heatmap(1-cor_mat,hclustfun = hclust_ave,
main=expression("Heatmap of "~hat(Omega)~"("*d[ij]*"=1-"*rho[ij]*")"))

# We save the heatmap for the Supplementary Information
pdf(width = 6, height = 6, here("figs/BayPass/SI_Heatmap.pdf"))
heatmap(1-cor_mat,hclustfun = hclust_ave)
dev.off()
```


# IS standard covariate model

## Mathematical model

The standard covariate model is represented in the following figure (**Figure 1B from the <span style="font-variant:small-caps;">BayPass</span> the manual**):


![](StandardModelBayPass.png)

From <span style="font-variant:small-caps;">BayPass</span> manual: 

  - 'This model allows evaluating to which extent a population covariable $k$ is (linearly) associated with each marker $i$ (which are assumed independent given $\Omega$) by the introduction of the regression coefficients $\beta_{ik}$ (for convenience the indices $k$ for covariables are dropped in Figure 1B).'


  - 'Importance Sampling (IS) approximation allows estimating Bayes Factor to evaluate the support in favor of association of each SNP $i$ with a covariable $k$, i.e., to compare the model with association ($\beta_{ik} \neq 0$) against the null model ($\beta_{ik} = 0$). The IS based estimation was initially proposed by Coop et al. (2010) and is based on a numerical integration that requires the definition of a grid covering the whole support of the $\beta_{ik}$ prior distribution. In <span style="font-variant:small-caps;">BayPass</span>, the grid consists of $n_{\beta}$ (by default $n_{\beta} = 201$) equidistant points from $\beta_{min}$ to $\beta_{max}$ (including the boundaries) leading to a lag between two successive values equal to $\frac{\beta_{max}-\beta_{min}}{n_{\beta}-1}$ (i.e., 0.003 with default values). Other values for $n_{\beta}$ might be supplied by the user with the `-nbetagrid` option.'


## Running the model

<span style="color: red;">We run the standard covariate model using Important sampling on the genomic dataset filtered for MAF.</span>


```{bash, eval=F}
# ** Bash script **
# =================

# Working directory before running the loop: data/BayPassAnalysis/outputs

for var in bio1 bio3 bio4 bio12 bio15 SHM
do
cd ISruns_${var}
for seed in {1..5}
do
~/Bureau/baypass_2.2/sources/g_baypass \
    -gfile ../../GenomicInputBayPass_withoutmaf.txt \
    -efile ../../EnvironmentalVariables/${var}.txt\
    -omegafile ../anacore_mat_omega.out \
    -outprefix anacovis${seed} \
    -seed ${seed}
done
cd ..
done
```

```{r ExtractSummaryOutputs}
# Number of iterations
nb_iter <- 5

# extract SNP names
snp_names <- read.csv(here(paste0("data/DryadRepo/ImputedGenomicData_AlleleCounts_withoutmaf.csv")),
                          row.names = 1) %>% 
  rownames()

list_out <- lapply(clim_var, function(var){

# Bayes factors
BF <- lapply(1:nb_iter, function(x){
  read.table(here(paste0("data/BayPassAnalysis/outputs/ISruns_",var,"/anacovis",x,"_summary_betai_reg.out")),
             h=T)$BF.dB.
}) %>% 
  setNames(paste0("BF",1:nb_iter)) %>% 
  as_tibble()

# Regression coefficients Beta_is
beta <- lapply(1:nb_iter, function(x){
  read.table(here(paste0("data/BayPassAnalysis/outputs/ISruns_",var,"/anacovis",x,"_summary_betai_reg.out")),
             h=T)$Beta_is
}) %>% 
  setNames(paste0("beta",1:nb_iter)) %>% 
  as.data.frame()


# Empirical Bayesian P-values eBPis
eBP <- lapply(1:nb_iter, function(x){
  read.table(here(paste0("data/BayPassAnalysis/outputs/ISruns_",var,"/anacovis",x,"_summary_betai_reg.out")),
             h=T)$eBPis
}) %>% 
  setNames(paste0("eBP",1:nb_iter)) %>% 
  as.data.frame()

out <- tibble(snp=snp_names,
              medianBF=apply(BF,1,median),
              medianBeta=apply(beta,1,median),
              medianEBP=apply(eBP,1,median)) %>% 
  mutate(variable=var)

eBP_out <- tibble("eBP_threshold"=c(3),
                  {{var}} :=c(length(which(out$medianEBP>3))))

BF_out <-tibble("BF_threshold" = c(20,10,5),
                {{var}} :=c(length(which(out$medianBF>20)),
                            length(which(out$medianBF>10)),
                            length(which(out$medianBF>5))))

list(snp_out = out,
     BF_out = BF_out,
     eBP_out = eBP_out,
     cor_BF = cor(BF),
     cor_beta = cor(beta),
     cor_eBP = cor(eBP))
}) %>% setNames(clim_var)
```



## Checking convergence across runs

Checking convergence by running several independent runs:

  - From <span style="font-variant:small-caps;">BayPass</span> manual: 'As for any MCMC analysis, it is recommended to run several independent MCMC (e.g., from 3 to 5), using different seeds for the random number generators (see `-seed` option). Comparing the estimates of parameters like $\Omega$ and statistics like $XtX$ or BF across runs allows ensuring (empirically) that the chains properly converged. For large enough data sets, estimations are generally reproducible for most parameters and statistics. Yet, for measures like the BF$_{is}$ that are based on an Importance Sampling approximation, single run estimations may be unstable (in particular when the number of populations is small), it is then recommended to use as an estimate the median computed over several different independent runs. For a real life example, see @gautier2018genomic.'

  - In @gautier2018genomic: 'Three independent runs (using the option `-seed`) were performed for each dataset. The estimated model hyper-parameters were highly consistent across both runs and datasets. Support for association of each SNP with the corresponding prevalence covariate was then evaluated using the median Bayes Factor (BF) computed over the three independent runs.'

<span style="color: red;">To check convergence, we run **5 independent runs** (different `-seed` options) for each climatic variable. Below, we use the median of the Bayes factors ($BF$) and of the empirical p-values ($eBP$) to identify the candidate SNPs.</span>

```{r Corrplots, fig.height=5,fig.width=14, results="hide"}
par(mfrow=c(1,3))

lapply(clim_var, function(var){
  
  lapply(c("BF","beta","eBP"), function(x){
    
list_out[[var]][[paste0("cor_",x)]] %>% corrplot(method = 'number',
                                                 type = 'lower', 
                                                 diag = FALSE,
                                                 title=paste0(var," - ",x),
                                                 mar=c(0,0,2,0),
                                                 number.cex=2,
                                                 tl.cex=1.5)
  })
})
```



## Identification threshold

Which threshold do we use to identify the candidate SNPs?

From @gautier2015genome: 'Jeffreys’rule (Jeffreys 1961) provides a useful decision criterion to quantify the strength of evidence (here in favor of association of the SNP with the covariable), using the following dB unit scale: 'strong evidence' when $10 < BF < 15$, 'very strong evidence' when $15 < BF < 20$, and 'decisive evidence' when $BF > 20$.'

  - @gautier2015genome uses $BF > 20$ and $eBP > 3$.

  - @vieira2022comparative uses $eBFmc > 3$ (using MCMC algorithm).
  
  - @ahrens2019standing uses $eBPis > 3$ (using IS algorithm).
  
  - @mayol2020multiscale uses $BF > 30$.

  - @pina2019new uses $BF > 15$ (using the AUX model).
  
  - @ruiz2019looking uses $eBPis > 3$.
  
  - @vendrami2019rad $BF > 10$ (using standard covariate model)

Here is the number of candidates using different thresholds ($BF > 5$, $BF > 10$,$BF > 20$ or $eBP > 3$) for each climatic variable:

```{r NumberCandidates}
list_out %>% lapply(function(out)out$BF_out) %>%  
  reduce(left_join, by = "BF_threshold") %>% 
  kable_mydf()

list_out %>% lapply(function(out)out$eBP_out) %>%  
  reduce(left_join, by = "eBP_threshold") %>% 
  kable_mydf()
```

Threshold used for the candidate SNPs:
  
```{r IdentificationThreshold}
threshold <- 10 # in dB
```

Given this threshold, here are the candidate SNPs:

```{r IdentifyCandidatesGivenThreshold}
cand <- list_out %>% 
  lapply(function(out)out$snp_out[which(out$snp_out$medianBF>threshold),]) %>%  
  bind_rows()

cand %>% kable_mydf()
```

Are there any SNPs associated with several climatic variables?

```{r IdentifyDuplicates}
# Identify duplicates
cand %>% group_by(snp) %>% filter(n()>1) %>% kable_mydf()
```

We export the list of candidate SNPs.

```{r ExportCandidateSNPs}
unique(cand$snp) %>% saveRDS(here("outputs/BayPass/baypass_outliers.rds"))
```

